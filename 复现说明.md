# SafeLoRA 论文复现说明（中文）

本文档给出一个可直接运行的复现流程，包含：

1. 使用 `datasets/samsum_1000_bad.jsonl` 重新进行 LoRA 微调。
2. 对微调后 LoRA 权重应用 SafeLoRA 投影。
3. 在 `datasets/samsum_test.jsonl` 上评估 ROUGE。
4. 使用一键脚本串联全流程。

---

## 一、环境准备

### 1) Python 环境

建议 Python 3.10+。

```powershell
python -m venv .venv
.\.venv\Scripts\Activate.ps1
python -m pip install -U pip
python -m pip install -r requirements-reproduce.txt
```

### 2) 模型准备

你需要准备同系列模型：

- `Base Model`（未对齐）：例如 Llama-2-7b-base
- `Chat/Instruct Model`（已对齐）：例如 Llama-2-7b-chat

并确保两者架构对应，且你有本地路径或可访问 HuggingFace 路径。

---

## 二、脚本说明

### 1) LoRA 微调脚本

文件：`scripts/train_samsum_lora.py`

功能：读取 `datasets/samsum_1000_bad.jsonl`，将 `messages` 转为 SFT 训练样本，按 LoRA 设置训练并保存 adapter。

关键默认参数（与论文常见设置对齐）：

- `target_modules=q_proj,v_proj`
- `lora_r=8`
- `learning_rate=5e-5`
- `num_train_epochs=5`

### 2) SafeLoRA 投影脚本

文件：`scripts/apply_safelora.py`

功能：加载微调得到的 LoRA adapter，结合 `base_model` 与 `chat_model` 计算对齐矩阵并执行投影，输出修正后的 adapter。

默认示例参数：

- `select_layers_type=number`
- `num_proj_layers=7`
- `threshold=0.35`

### 3) 评估脚本

文件：`scripts/eval_samsum_rouge.py`

功能：加载（Safe）LoRA adapter，在 `datasets/samsum_test.jsonl` 上推理并统计 ROUGE。

---

## 三、分步手动复现

下面示例请替换为你的真实模型路径：

- `D:/models/Llama-2-7b-base`
- `D:/models/Llama-2-7b-chat`

### Step 1: LoRA 微调

```powershell
python scripts/train_samsum_lora.py `
  --chat_model_path D:/models/Llama-2-7b-chat `
  --train_file datasets/samsum_1000_bad.jsonl `
  --output_dir outputs/lora_samsum_bad `
  --num_train_epochs 5 `
  --learning_rate 5e-5 `
  --lora_r 8 `
  --target_modules q_proj,v_proj `
  --fp16
```

### Step 2: 应用 SafeLoRA

```powershell
python scripts/apply_safelora.py `
  --chat_model_path D:/models/Llama-2-7b-chat `
  --base_model_path D:/models/Llama-2-7b-base `
  --adapter_path outputs/lora_samsum_bad `
  --output_adapter_path outputs/lora_samsum_bad_safelora `
  --select_layers_type number `
  --num_proj_layers 7 `
  --threshold 0.35 `
  --fp16
```

### Step 3: 评估 ROUGE

```powershell
python scripts/eval_samsum_rouge.py `
  --chat_model_path D:/models/Llama-2-7b-chat `
  --adapter_path outputs/lora_samsum_bad_safelora `
  --test_file datasets/samsum_test.jsonl `
  --max_samples 200 `
  --fp16
```

---

## 四、一键运行

文件：`scripts/run_reproduce.sh`

此脚本会自动执行：依赖安装 → LoRA 微调 → SafeLoRA 投影 → ROUGE 评估。

脚本内已默认配置：

- Base 模型：`/data/xieqiuhao/tjy/downloaded_models/Llama-2-7b-hf`
- Chat 模型：`/data/xieqiuhao/tjy/downloaded_models/Llama-2-7b-chat-hf`

```powershell
bash scripts/run_reproduce.sh
```

如需改参数（训练轮数、LoRA rank、阈值、输出目录等），直接修改 `scripts/run_reproduce.sh` 顶部 `Default config` 区域即可。

---

## 五、结果与注意事项

1. 训练和推理显存开销较大，建议使用 GPU（`--fp16` 或 `--bf16`）。
2. SafeLoRA 要求 base/chat 模型同源且参数维度一一对应。
3. 论文中的绝对数值会受模型版本、数据清洗、随机种子和硬件影响，建议多次运行统计均值。
